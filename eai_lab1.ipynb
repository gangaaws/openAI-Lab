{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN2x+85Ho+3E5ScRn/sYuuh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gangaaws/openAI-Lab/blob/main/eai_lab1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "nKpfVSQ-E9wK"
      },
      "outputs": [],
      "source": [
        "# Install the OpenAI library\n",
        "!pip install openai --quiet\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "from openai import OpenAI\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6mrFwhUCJOSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#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\n"
      ],
      "metadata": {
        "id": "BiIo73UtHE2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Secure way to add your API key in Colab\n",
        "from getpass import getpass\n",
        "\n",
        "# This will create a password field to enter your API key securely\n",
        "api_key = getpass('Enter your OpenAI API key: ')\n",
        "\n",
        "# Initialize the OpenAI client\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n",
        "print(\"‚úÖ OpenAI client initialized successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xh6bp9HhFnc0",
        "outputId": "4b9006f9-a03b-468c-cace-19d8dfa30704"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your OpenAI API key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "‚úÖ OpenAI client initialized successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic text generation example\n",
        "response = client.responses.create(\n",
        "    model=\"gpt-4o\",\n",
        "    input=\"Write a one-sentence bedtime story about a unicorn.\"\n",
        ")\n",
        "\n",
        "# Display the generated text\n",
        "print(\"Generated Story:\")\n",
        "print(response.output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROdSV9uFFtwQ",
        "outputId": "3a859762-ee57-41e6-9f62-90eb024f4dc9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Story:\n",
            "In a moonlit meadow, a gentle unicorn named Luna sprinkled stardust with her shimmering horn, bringing sweet dreams to all the sleeping creatures.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine the full response object\n",
        "print(\"Response Type:\", type(response))\n",
        "print(\"\\nAvailable Attributes:\")\n",
        "print(dir(response))\n",
        "\n",
        "# Access specific response metadata\n",
        "if hasattr(response, 'usage'):\n",
        "    print(\"\\nToken Usage:\")\n",
        "    print(f\"  Input tokens: {response.usage.input_tokens}\")\n",
        "    print(f\"  Output tokens: {response.usage.output_tokens}\")\n",
        "    print(f\"  Total tokens: {response.usage.total_tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaQrxSXFF46J",
        "outputId": "00c0f9a3-821d-4c5c-d018-9c3871c3eb86"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response Type: <class 'openai.types.responses.response.Response'>\n",
            "\n",
            "Available Attributes:\n",
            "['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_setattr_handlers__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_copy_and_set_values', '_get_value', '_iter', '_request_id', '_setattr_handler', 'background', 'construct', 'conversation', 'copy', 'created_at', 'dict', 'error', 'from_orm', 'id', 'incomplete_details', 'instructions', 'json', 'max_output_tokens', 'max_tool_calls', 'metadata', 'model', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'object', 'output', 'output_text', 'parallel_tool_calls', 'parse_file', 'parse_obj', 'parse_raw', 'previous_response_id', 'prompt', 'prompt_cache_key', 'reasoning', 'safety_identifier', 'schema', 'schema_json', 'service_tier', 'status', 'temperature', 'text', 'to_dict', 'to_json', 'tool_choice', 'tools', 'top_logprobs', 'top_p', 'truncation', 'update_forward_refs', 'usage', 'user', 'validate']\n",
            "\n",
            "Token Usage:\n",
            "  Input tokens: 18\n",
            "  Output tokens: 30\n",
            "  Total tokens: 48\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Advanced parameters example\n",
        "def generate_text(prompt, temperature=0.7, max_output_tokens=150, top_p=1.0):\n",
        "    \"\"\"\n",
        "    Generate text with customizable parameters\n",
        "\n",
        "    Args:\n",
        "        prompt: Input text for the model\n",
        "        temperature: Controls randomness (0.0 to 2.0)\n",
        "        max_output_tokens: Maximum length of generated text\n",
        "        top_p: Nucleus sampling parameter\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Assuming 'client' is an initialized OpenAI client\n",
        "        response = client.responses.create(\n",
        "            model=\"gpt-4o\",\n",
        "            input=prompt,\n",
        "            temperature=temperature,\n",
        "            max_output_tokens=max_output_tokens,\n",
        "            top_p=top_p\n",
        "        )\n",
        "        return response.output_text\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "# Test with different temperatures\n",
        "prompt = \"Explain quantum computing in simple terms:\"\n",
        "\n",
        "print(\"üå°Ô∏è Low Temperature (0.3) - More Focused:\")\n",
        "# Pass the parameter as max_output_tokens\n",
        "print(generate_text(prompt, temperature=0.3, max_output_tokens=100))\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "print(\"üå°Ô∏è High Temperature (1.5) - More Creative:\")\n",
        "# Pass the parameter as max_output_tokens\n",
        "print(generate_text(prompt, temperature=1.5, max_output_tokens=100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfVzc_xXF-1m",
        "outputId": "023f2787-3877-49b3-e471-84fe9451b31b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üå°Ô∏è Low Temperature (0.3) - More Focused:\n",
            "Sure! Quantum computing is a type of computing that uses the principles of quantum mechanics to process information. Here's a simple breakdown:\n",
            "\n",
            "1. **Bits vs. Qubits**: Traditional computers use bits as the smallest unit of data, which can be either 0 or 1. Quantum computers use qubits, which can be 0, 1, or both at the same time, thanks to a property called superposition.\n",
            "\n",
            "2. **Superposition**: This allows quantum computers to process\n",
            "\n",
            "==================================================\n",
            "\n",
            "üå°Ô∏è High Temperature (1.5) - More Creative:\n",
            "Sure! Quantum computing is a different style of computing that uses the principles of quantum mechanics, which is the science of the very small, like atoms and subatomic particles.\n",
            "\n",
            "Traditional computers use bits, which are ones and zeros, to process information. Think of each bit like a light switch that can be either on or off.\n",
            "\n",
            "Quantum computers, on the other hand, use qubits. Qubits can be both on (1) and off (0) at the same time through a property known\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch processing example\n",
        "prompts = [\n",
        "    \"Write a haiku about coding\",\n",
        "    \"Create a recipe title for a futuristic dish\",\n",
        "    \"Suggest a name for a friendly robot\"\n",
        "]\n",
        "\n",
        "results = []\n",
        "\n",
        "for i, prompt in enumerate(prompts, 1):\n",
        "    print(f\"Processing prompt {i}/{len(prompts)}...\")\n",
        "\n",
        "    # Corrected the keyword argument from max_tokens to max_output_tokens\n",
        "    response = client.responses.create(\n",
        "        model=\"gpt-4o\",\n",
        "        input=prompt,\n",
        "        temperature=0.8,\n",
        "        max_output_tokens=50\n",
        "    )\n",
        "\n",
        "    results.append({\n",
        "        \"prompt\": prompt,\n",
        "        \"response\": response.output_text\n",
        "    })\n",
        "\n",
        "# Display results\n",
        "print(\"\\nüìä Batch Processing Results:\\n\")\n",
        "for result in results:\n",
        "    print(f\"Prompt: {result['prompt']}\")\n",
        "    print(f\"Response: {result['response']}\")\n",
        "    print(\"-\" * 40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yMLuVwhNRW1",
        "outputId": "31b03987-d73f-4cdb-d4d6-88f957e7d274"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing prompt 1/3...\n",
            "Processing prompt 2/3...\n",
            "Processing prompt 3/3...\n",
            "\n",
            "üìä Batch Processing Results:\n",
            "\n",
            "Prompt: Write a haiku about coding\n",
            "Response: Lines of logic flow,  \n",
            "Silent dance of ones and zeros‚Äî  \n",
            "Creation unfolds.\n",
            "----------------------------------------\n",
            "Prompt: Create a recipe title for a futuristic dish\n",
            "Response: Quantum Fusion Sushi Rolls\n",
            "----------------------------------------\n",
            "Prompt: Suggest a name for a friendly robot\n",
            "Response: Sure! How about \"BuddyBot\"?\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def safe_generate(prompt, retries=3, delay=1):\n",
        "    \"\"\"\n",
        "    Generate text with retry logic and error handling\n",
        "\n",
        "    Args:\n",
        "        prompt: Input text\n",
        "        retries: Number of retry attempts\n",
        "        delay: Delay between retries (seconds)\n",
        "    \"\"\"\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            # Corrected parameter from max_tokens to max_output_tokens\n",
        "            response = client.responses.create(\n",
        "                model=\"gpt-4o\",\n",
        "                input=prompt,\n",
        "                temperature=0.7,\n",
        "                max_output_tokens=100  # <--- This line is changed\n",
        "            )\n",
        "            return {\n",
        "                \"success\": True,\n",
        "                \"text\": response.output_text,\n",
        "                \"attempt\": attempt + 1\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
        "            if attempt < retries - 1:\n",
        "                print(f\"Retrying in {delay} seconds...\")\n",
        "                time.sleep(delay)\n",
        "            else:\n",
        "                return {\n",
        "                    \"success\": False,\n",
        "                    \"error\": str(e),\n",
        "                    \"attempts\": retries\n",
        "                }\n",
        "\n",
        "# Test the function\n",
        "result = safe_generate(\"What are three benefits of cloud computing?\")\n",
        "\n",
        "if result[\"success\"]:\n",
        "    print(\"\\n‚úÖ Generation successful!\")\n",
        "    print(f\"Response: {result['text']}\")\n",
        "else:\n",
        "    print(\"\\n‚ùå Generation failed after all retries\")\n",
        "    print(f\"Error: {result['error']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wL2WzRVN1Z2",
        "outputId": "375e2781-4380-4c8b-9194-8206741cfa6a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Generation successful!\n",
            "Response: Cloud computing offers several benefits, including:\n",
            "\n",
            "1. **Scalability**: Cloud services allow businesses to easily scale their resources up or down based on demand without the need for significant upfront investments in hardware. This flexibility helps accommodate changing workloads efficiently.\n",
            "\n",
            "2. **Cost Efficiency**: By using cloud services, companies can reduce the costs associated with maintaining physical servers and infrastructure. They only pay for the resources they use, which can lead to significant savings on IT expenses.\n",
            "\n",
            "3. **Accessibility and Collaboration**\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#part 5\n",
        "def creative_assistant(topic, style=\"neutral\", min_words=50, max_words=100):\n",
        "    \"\"\"\n",
        "    Your creative writing assistant\n",
        "\n",
        "    Args:\n",
        "        topic: What to write about\n",
        "        style: Writing style (poetic/technical/humorous/neutral)\n",
        "        min_words: Minimum word count\n",
        "        max_words: Maximum word count\n",
        "\n",
        "    Returns:\n",
        "        Generated text in the specified style\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: Define style parameters (temperature, prompts)\n",
        "    styles = {\n",
        "        \"poetic\": {\n",
        "            \"temperature\": 1.2,\n",
        "            \"prefix\": \"Write poetically about: \"\n",
        "        },\n",
        "        # Add more styles here\n",
        "    }\n",
        "\n",
        "    # TODO: Implement the generation logic\n",
        "\n",
        "    # TODO: Check word count and regenerate if needed\n",
        "\n",
        "    pass  # Replace with your implementation\n",
        "\n",
        "#Test your function\n",
        "result = creative_assistant(\"artificial intelligence\", style=\"poetic\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8rLQT93Ofq2",
        "outputId": "9af5de7a-8bba-4905-83d0-38e884eec508"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "lnPQZ5lIPXyE"
      }
    }
  ]
}